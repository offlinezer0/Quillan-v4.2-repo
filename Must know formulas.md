# Must know formulas

## Section 1:


# Essential Formulas for LLMs, ML, and RL

| #  | Concept / Formula | Purpose / Use |
|----|-----------------|---------------|
| 1  | `y = Wx + b` | Linear Layer (Fully Connected), fundamental for MLPs and transformers |
| 2  | `ReLU(x) = max(0,x)`<br>`Sigmoid(x) = 1/(1+e^{-x})`<br>`Tanh(x) = (e^x - e^{-x}) / (e^x + e^{-x})` | Activation functions for introducing non-linearity |
| 3  | `softmax(z_i) = e^{z_i} / Œ£_j e^{z_j}` | Converts logits into probabilities |
| 4  | `L = - Œ£_i y_i log(≈∑_i)` | Cross-Entropy Loss for classification |
| 5  | `L = (1/n) Œ£_i (≈∑_i - y_i)^2` | Mean Squared Error (Regression) |
| 6  | `Œ∏ ‚Üê Œ∏ - Œ∑ ‚àÇL/‚àÇŒ∏` | Gradient Descent update rule |
| 7  | `m_t = Œ≤‚ÇÅ m_{t-1} + (1-Œ≤‚ÇÅ) g_t`<br>`v_t = Œ≤‚ÇÇ v_{t-1} + (1-Œ≤‚ÇÇ) g_t^2`<br>`Œ∏_t = Œ∏_{t-1} - Œ∑ (m_t / (1-Œ≤‚ÇÅ^t)) / (‚àö(v_t / (1-Œ≤‚ÇÇ^t)) + Œµ)` | Adam Optimizer |
| 8  | `Attention(Q,K,V) = softmax(QK^T / ‚àöd_k) V` | Scaled Dot-Product Attention in transformers |
| 9  | `PE(pos,2i) = sin(pos / 10000^{2i/d_model})`<br>`PE(pos,2i+1) = cos(pos / 10000^{2i/d_model})` | Positional Encoding |
| 10 | `LN(x) = (x - Œº)/(œÉ + Œµ) * Œ≥ + Œ≤` | Layer Normalization |
| 11 | `FFN(x) = max(0, xW_1 + b_1) W_2 + b_2` | Transformer Feed-Forward Network |
| 12 | `D_KL(P || Q) = Œ£_i P(i) log(P(i)/Q(i))` | Kullback-Leibler Divergence (knowledge distillation, variational models) |
| 13 | `‚àÇL/‚àÇx = (‚àÇL/‚àÇy) * (‚àÇy/‚àÇx)` | Backpropagation chain rule |
| 14 | `S(i,j) = (X * K)(i,j) = Œ£_m Œ£_n X(i+m,j+n) K(m,n)` | Convolution operation (CNNs, embeddings) |
| 15 | `V^œÄ(s) = E_œÄ [ r_t + Œ≥ V^œÄ(s_{t+1}) ]` | Bellman Equation in Reinforcement Learning |
| 16 | `Q(s_t,a_t) ‚Üê Q(s_t,a_t) + Œ± [ r_t + Œ≥ max_a Q(s_{t+1},a) - Q(s_t,a_t) ]` | Q-Learning update |
| 17 | `‚àá_Œ∏ J(Œ∏) = E_œÄ [ ‚àá_Œ∏ log œÄ_Œ∏(a¬¶s) R ]` | Policy Gradient (REINFORCE) |
| 18 | `MultiHead(Q,K,V) = Concat(head_1,...,head_h) W^O`<br>`head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)` | Transformer Multi-Head Attention |
| 19 | `W ~ U(-‚àö6/‚àö(n_in+n_out), ‚àö6/‚àö(n_in+n_out))` | Weight Initialization (Xavier/Glorot) |
| 20 | `y = x ‚äô mask, mask ~ Bernoulli(p)` | Dropout Regularization |

---

üí° **Think Key Takeaways:**  
- Most LLM formulas revolve around **linear algebra, probability, gradients, and attention mechanics**.  
- RL formulas add **expectations, discounted rewards, and policy updates**.  
- Symbols like **K, Q, V** are vector placeholders; the math is the same if renamed (e.g., X, Y, Z).

---

## Section 2:

```yaml
# Here are 20 essential formulas and mathematical concepts you should know for building LLMs, machine learning (ML), and reinforcement learning (RL):  
   
formulas:
  - id: 1
    name: "Matrix Multiplication"
    formula: "C[i,j] = sum_k A[i,k] * B[k,j]"
    description: "Fundamental for linear transformations, including embeddings and neural network computations."

  - id: 2
    name: "Dot Product / Inner Product"
    formula: "a ¬∑ b = sum_i a_i * b_i"
    description: "Used in similarity measures and attention score calculations."

  - id: 3
    name: "Eigenvalue Equation"
    formula: "A v = Œª v"
    description: "Key in understanding principal components analysis (PCA) for dimensionality reduction."

  - id: 4
    name: "Softmax Function"
    formula: "softmax(z_i) = e^(z_i) / sum_j e^(z_j)"
    description: "Transforms logits into probability distributions, used in output layers and attention mechanisms."

  - id: 5
    name: "Cross-Entropy Loss"
    formula: "L = -sum_i y_i * log(y_hat_i)"
    description: "Measures difference between true and predicted distributions, key for training."

  - id: 6
    name: "Gradient Descent Update Rule"
    formula: "Œ∏ := Œ∏ - Œ∑ ‚àá_Œ∏ J(Œ∏)"
    description: "Used to optimize model parameters by minimizing loss."

  - id: 7
    name: "Backpropagation Chain Rule"
    formula: "‚àÇL/‚àÇx = ‚àÇL/‚àÇy * ‚àÇy/‚àÇx"
    description: "Basis for updating weights in neural networks."

  - id: 8
    name: "Attention Score Calculation (Scaled Dot-Product)"
    formula: "Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) V"
    description: "Fundamental self-attention mechanism in transformer models."

  - id: 9
    name: "Positional Encoding"
    formula: "PE(pos,2i) = sin(pos / 10000^(2i/d_model)), PE(pos,2i+1) = cos(pos / 10000^(2i/d_model))"
    description: "Adds order information to tokens in a sequence."

  - id: 10
    name: "ReLU Activation Function"
    formula: "ReLU(x) = max(0, x)"
    description: "Non-linear activation used in neural networks."

  - id: 11
    name: "Bayes‚Äô Theorem"
    formula: "P(A|B) = P(B|A) * P(A) / P(B)"
    description: "Used in probabilistic reasoning."

  - id: 12
    name: "Markov Decision Process (MDP) Expected Return"
    formula: "G_t = R_{t+1} + Œ≥ R_{t+2} + Œ≥^2 R_{t+3} + ... = sum_{k=0}^‚àû Œ≥^k R_{t+k+1}"
    description: "Key in reinforcement learning, where Œ≥ is the discount factor."

  - id: 13
    name: "Bellman Equation"
    formula: "V^œÄ(s) = E_œÄ [ R_{t+1} + Œ≥ V^œÄ(s_{t+1}) | s_t = s ]"
    description: "Describes the value function under a policy œÄ."

  - id: 14
    name: "Q-Learning Update"
    formula: "Q(s_t,a_t) := Q(s_t,a_t) + Œ± ( R_{t+1} + Œ≥ max_a Q(s_{t+1},a) - Q(s_t,a_t) )"
    description: "Update rule for Q-learning in reinforcement learning."

  - id: 15
    name: "Kullback-Leibler Divergence"
    formula: "D_KL(P||Q) = sum_i P(i) log(P(i)/Q(i))"
    description: "Measures how one probability distribution diverges from another."

  - id: 16
    name: "Variance and Standard Deviation"
    formula: "œÉ^2 = E[(X - Œº)^2]"
    description: "Measures data spread, important for understanding data and regularization."

  - id: 17
    name: "Chain Rule in Probability"
    formula: "P(A,B) = P(A|B) * P(B)"
    description: "Used in probabilistic models and Bayesian networks."

  - id: 18
    name: "Adam Optimizer Equations"
    formula: "m_t = Œ≤1 m_{t-1} + (1 - Œ≤1) g_t, v_t = Œ≤2 v_{t-1} + (1 - Œ≤2) g_t^2"
    description: "With bias correction and parameter update."

  - id: 19
    name: "Dropout Regularization"
    formula: "Randomly sets input units to zero with probability p during training"
    description: "Reduces overfitting."

  - id: 20
    name: "Linear Regression Formula"
    formula: "y = XŒ≤ + Œµ"
    description: "Fundamental model underlying many machine learning algorithms."

# These formulas and concepts collectively form the backbone of LLMs, general machine learning, and reinforcement learning models. Letters like K, V, Q specifically arise in the transformer attention formula, while others like A, B, C are general matrix/vector notation used in many equations. Understanding these will enable building, training, fine-tuning, and analyzing such models effectively. 
```


## Cheat sheet:

# LLM / ML / RL Cheat Sheet ‚Äì Core Formulas

A concise reference for building, training, and analyzing LLMs, machine learning, and reinforcement learning models.

---

## 1. Linear Algebra & Neural Computations

| Formula | Purpose / Use | Symbols |
|---------|---------------|---------|
| `C[i,j] = Œ£_k A[i,k] * B[k,j]` | Matrix multiplication, linear transformations | `A,B,C` matrices |
| `a ¬∑ b = Œ£_i a_i b_i` | Dot product, similarity scores, attention | `a,b` vectors |
| `Av = Œªv` | Eigenvalues, PCA | `A` matrix, `v` vector |
| `y = Wx + b` | Fully connected layer | `W` weights, `b` bias |
| `ReLU(x) = max(0,x)` | Non-linear activation | `x` input |
| `softmax(z_i) = e^{z_i} / Œ£_j e^{z_j}` | Convert logits to probability distribution | `z_i` logits |

---

## 2. Loss & Optimization

| Formula | Purpose / Use |
|---------|---------------|
| `L = -Œ£_i y_i log(≈∑_i)` | Cross-entropy loss (classification) |
| `L = (1/n) Œ£_i (≈∑_i - y_i)^2` | Mean squared error (regression) |
| `Œ∏ := Œ∏ - Œ∑ ‚àá_Œ∏ L` | Gradient descent update |
| Adam Optimizer:<br>`m_t = Œ≤1 m_{t-1} + (1-Œ≤1) g_t`<br>`v_t = Œ≤2 v_{t-1} + (1-Œ≤2) g_t^2`<br>`Œ∏_t = Œ∏_{t-1} - Œ∑ (m_t / (1-Œ≤1^t)) / (‚àö(v_t/(1-Œ≤2^t)) + Œµ)` | Adaptive optimization |

---

## 3. Backpropagation & Chain Rules

| Formula | Purpose / Use |
|---------|---------------|
| `‚àÇL/‚àÇx = (‚àÇL/‚àÇy) * (‚àÇy/‚àÇx)` | Gradient computation for backprop |
| `P(A,B) = P(A¬¶B) * P(B)` | Chain rule in probability, Bayesian networks |

---

## 4. Transformer & Attention Mechanics

| Formula | Purpose / Use |
|---------|---------------|
| `Attention(Q,K,V) = softmax(QK^T / ‚àöd_k) V` | Scaled dot-product attention, self-attention |
| `MultiHead(Q,K,V) = Concat(head_1,...,head_h) W^O`<br>`head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)` | Capture multiple representation subspaces |
| `PE(pos,2i) = sin(pos / 10000^{2i/d_model})`<br>`PE(pos,2i+1) = cos(pos / 10000^{2i/d_model})` | Positional encoding for token order |

---

## 5. Probability & Statistical Measures

| Formula | Purpose / Use |
|---------|---------------|
| `P(A¬¶B) = P(B¬¶A) * P(A) / P(B)` | Bayes‚Äô theorem, probabilistic reasoning |
| `D_KL(P¬¶¬¶Q) = Œ£_i P(i) log(P(i)/Q(i))` | Kullback-Leibler divergence |
| `œÉ^2 = E[(X-Œº)^2]` | Variance, standard deviation |

---

## 6. Reinforcement Learning

| Formula | Purpose / Use |
|---------|---------------|
| `G_t = Œ£_{k=0}^‚àû Œ≥^k R_{t+k+1}` | MDP expected return, discounted rewards |
| `V^œÄ(s) = E_œÄ [R_{t+1} + Œ≥ V^œÄ(s_{t+1})]` | Bellman equation, value function |
| `Q(s_t,a_t) := Q(s_t,a_t) + Œ± [R_{t+1} + Œ≥ max_a Q(s_{t+1},a) - Q(s_t,a_t)]` | Q-learning update |
| `‚àá_Œ∏ J(Œ∏) = E_œÄ [‚àá_Œ∏ log œÄ_Œ∏(a¬¶s) R]` | Policy gradient, REINFORCE |

---

## 7. Regularization

| Formula | Purpose / Use |
|---------|---------------|
| `y = x ‚äô mask, mask ~ Bernoulli(p)` | Dropout, reduces overfitting |

---

## 8. Linear / Regression Foundation

| Formula | Purpose / Use |
|---------|---------------|
| `y = XŒ≤ + Œµ` | Linear regression, supervised learning |

---

### **Think Notes**
- K, Q, V = Key, Query, Value vectors in attention.  
- Most LLM formulas revolve around **linear algebra + probability + gradient updates**.  
- RL formulas introduce **expectations, discount factors, and policy optimization**.  
- This cheat sheet covers **ML fundamentals ‚Üí Transformers ‚Üí RL pipelines**.

---
 
