{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-","timestamp":1761276773966}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Building a GPT\n","\n","Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."],"metadata":{"id":"wJpXpmjEYC_T"}},{"cell_type":"markdown","source":["reverse engineering this for out own model"],"metadata":{"id":"fIQeGmrMn0ZX"}},{"cell_type":"code","source":["!git clone https://github.com/leeex1/Quillan-v4.2-repo.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-QfDcdPt0Rih","executionInfo":{"status":"ok","timestamp":1761276954642,"user_tz":240,"elapsed":3313,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"fe957741-5886-4c82-9c03-9bc21a7bdb3a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Quillan-v4.2-repo'...\n","remote: Enumerating objects: 2208, done.\u001b[K\n","remote: Counting objects: 100% (269/269), done.\u001b[K\n","remote: Compressing objects: 100% (230/230), done.\u001b[K\n","remote: Total 2208 (delta 186), reused 39 (delta 39), pack-reused 1939 (from 2)\u001b[K\n","Receiving objects: 100% (2208/2208), 42.12 MiB | 27.56 MiB/s, done.\n","Resolving deltas: 100% (1325/1325), done.\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","\n","# --- Paths ---\n","jsonl_file_path = \"/content/Quillan-v4.2-repo/Quillan-v4.2-model/Quillan_finetune_full_dataset.jsonl\"\n","quillan_files_dir = \"/content/Quillan-v4.2-repo/Quillan files\"\n","\n","# --- Function to recursively extract text/content from JSON objects (for .jsonl lines) ---\n","def find_text_in_json(data):\n","    texts = []\n","    if isinstance(data, str):\n","        texts.append(data)\n","    elif isinstance(data, dict):\n","        for key, value in data.items():\n","            texts.extend(find_text_in_json(value))\n","    elif isinstance(data, list):\n","        for item in data:\n","            texts.extend(find_text_in_json(item))\n","    return texts\n","\n","# --- Load main dataset file (.jsonl) ---\n","main_samples = []\n","if os.path.exists(jsonl_file_path):\n","    with open(jsonl_file_path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            try:\n","                data = json.loads(line.strip())\n","                # Recursively grab all string content\n","                main_samples.extend(find_text_in_json(data))\n","            except Exception:\n","                continue\n","else:\n","    print(f\"âŒ Could not find {jsonl_file_path}\")\n","\n","print(f\"Loaded {len(main_samples):,} samples from Quillan_finetune_full_dataset.jsonl\")\n","\n","# --- Load all text from every file in the Quillan directory ---\n","quillan_file_texts = []\n","for filename in os.listdir(quillan_files_dir):\n","    file_path = os.path.join(quillan_files_dir, filename)\n","    try:\n","        # Only use non-binary and non-image/text files (skip .gitkeep, images, etc.)\n","        if not filename.lower().endswith(('.png', '.jpg', 'jpeg', '.bmp', '.gif')):\n","            with open(file_path, \"r\", encoding=\"utf-8\") as fp:\n","                text = fp.read()\n","                # Only add meaningful content (skip tiny/empty)\n","                if len(text.strip()) > 10:\n","                    quillan_file_texts.append(text)\n","    except Exception as e:\n","        print(f\"Skipped {filename}: {e}\")\n","\n","print(f\"Loaded {len(quillan_file_texts)} files from Quillan folder.\")\n","\n","# --- FULL MERGED CORPUS ---\n","# Combine everything as a single string (or list of samples if batching line-by-line)\n","full_corpus = \"\\n\\n\".join(main_samples + quillan_file_texts)\n","\n","print(\"Merged corpus length (characters):\", len(full_corpus))\n","\n","# --- Tokenizer and batching as before ---\n","# At this point, run all splitting, tokenization, vocab building, batching, etc. on `full_corpus`\n","# Example:\n","chars = sorted(list(set(full_corpus)))\n","vocab_size = len(chars)\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s]\n","decode = lambda l: ''.join([itos[i] for i in l])\n","import torch\n","data = torch.tensor(encode(full_corpus), dtype=torch.long)\n","print(\"Tokens shape:\", data.shape)\n","\n","# Continue train/val split, batching, training, and model code as before...\n","# train_data, val_data = torch.split(data ...\n","# etc.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tLaQ_qJYtmH9","executionInfo":{"status":"ok","timestamp":1761277262982,"user_tz":240,"elapsed":4451,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"76ca4438-d372-43d9-de0e-6485257cde3b"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 432 samples from Quillan_finetune_full_dataset.jsonl\n","Loaded 57 files from Quillan folder.\n","Merged corpus length (characters): 8972889\n","Tokens shape: torch.Size([8972889])\n"]}]},{"cell_type":"code","source":["# Inspect the combined text content\n","# This variable was created in the previous cell by reading multiple files.\n","# Print the first 2500 characters to inspect it\n","print(f\"Full corpus length: {len(full_corpus)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-0gNGTN2tuLb","executionInfo":{"status":"ok","timestamp":1761277539461,"user_tz":240,"elapsed":55,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"061ca0ca-85d8-400e-8b00-75c2847b8ef8"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Full corpus length: 8972889\n"]}]},{"cell_type":"code","source":["# Train/validation split (90% train, 10% val)\n","n = int(0.9 * len(data))\n","train_data = data[:n]\n","val_data = data[n:]\n","print(\"Train tokens:\", train_data.shape)\n","print(\"Val tokens:\", val_data.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-jLpw-UQvMEU","executionInfo":{"status":"ok","timestamp":1761277543609,"user_tz":240,"elapsed":44,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"6580be45-0f76-4531-fa32-e600e3cabb68"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Train tokens: torch.Size([8075600])\n","Val tokens: torch.Size([897289])\n"]}]},{"cell_type":"code","source":["import torch\n","\n","batch_size = 32         # sequences per batch\n","block_size = 256        # max context length (matches model config)\n","\n","def get_batch(split):\n","    d = train_data if split == \"train\" else val_data\n","    ix = torch.randint(len(d) - block_size, (batch_size,))\n","    x = torch.stack([d[i:i+block_size] for i in ix])\n","    y = torch.stack([d[i+1:i+block_size+1] for i in ix])\n","    return x, y\n","\n","xb, yb = get_batch(\"train\")\n","print(\"Input shape:\", xb.shape)\n","print(\"Target shape:\", yb.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EZSDmvhfvTV-","executionInfo":{"status":"ok","timestamp":1761277545186,"user_tz":240,"elapsed":9,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"9084e299-b130-4dec-86b2-b92e46caff7c"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape: torch.Size([32, 256])\n","Target shape: torch.Size([32, 256])\n"]}]},{"cell_type":"code","source":["# here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","print(''.join(chars))\n","print(vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ceFRqKh-vZpi","executionInfo":{"status":"ok","timestamp":1761277564910,"user_tz":240,"elapsed":116,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"2dbac738-b253-4b50-af6e-c9ef00e5663e"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["\t\n"," !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~Â Â§Â«Â¬Â­Â±Â²Â·Â»Ã“Ã—Ã¡Ã¤Ã¦Ã©Ã­Ã¯Ã³Ã´Ã¶Ã¼ÄÅŒÅÊ¹Î“Î”Î˜ÎÎ£Î¦Î¨Î©Î±Î²Î³Î´Î¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏ‚ÏƒÏ„Ï†Ï‡ÏˆÏ‰ÏŒÏ•Ïµâ€‚â€ƒâ€…â€‰â€Šâ€‹â€â€â€‘â€“â€”â€˜â€™â€œâ€â€ â€¢â€¦â€ªâ€¬â¡â‚â‚‚â‚¬â„â„°â†‘â†’â†“â†”âˆˆâˆâˆ‘âˆ’âˆ™âˆšâˆ£âˆ¥âˆ«â‰ˆâ‰ â‰¥â‰²â‰»âŠ—â‹¯â°â±â”€â–¡â—‡â—‹â—â˜ â™€â™‚â™ â™£â™¥â™¦â™¾â™¿âš–âš™âš âš¡âœ…âœâœ“âœ—âœ¨âŒâ¤â”â¡âŸ©â¨‚â­ã€‰ã€ã€‘æ‰®æ¼”ï¸ï»¿ğ¸ğ‘ğ‘‚ğ‘†ğ‘ğ‘ğ‘‘ğ‘’ğ‘“ğ‘”ğ‘–ğ‘—ğ‘˜ğ‘™ğ‘šğ‘›ğ‘œğ‘ğ‘Ÿğ‘ ğ‘¡ğ‘¢ğ‘£ğ‘¤ğ‘¥ğ‘¦ğ‘§ğ–«ğ–¬ğ›¼ğ›½ğ›¾ğ›¿ğœğœ‚ğœƒğœ…ğœ†ğœ‡ğœ–ğœ™ğŸŒ€ğŸŒˆğŸŒ‰ğŸŒŠğŸŒŒğŸŒğŸŒğŸŒ™ğŸŒŸğŸŒªğŸŒ±ğŸŒ³ğŸ‰ğŸ¨ğŸªğŸ­ğŸ®ğŸ¯ğŸµğŸƒğŸ†ğŸğŸ—ğŸ›ğŸ¹ğŸ‘ğŸ‘‹ğŸ‘‘ğŸ‘¤ğŸ‘¥ğŸ’€ğŸ’ğŸ’–ğŸ’ğŸ’¡ğŸ’ªğŸ’«ğŸ’¬ğŸ’­ğŸ’¯ğŸ’»ğŸ’¾ğŸ“ˆğŸ“ŠğŸ“‹ğŸ“ŒğŸ“–ğŸ“˜ğŸ“šğŸ“œğŸ“ğŸ“¤ğŸ”€ğŸ”ğŸ”„ğŸ”ğŸ”‘ğŸ”’ğŸ”—ğŸ”šğŸ”¢ğŸ”¥ğŸ”§ğŸ”¬ğŸ”®ğŸ”¹ğŸ•°ğŸ•³ğŸ•¸ğŸ—£ğŸ—ºğŸ˜ŠğŸš€ğŸš¦ğŸš¨ğŸšªğŸš«ğŸš¶ğŸ› ğŸ›¡ğŸ›¤ğŸ¤”ğŸ¤–ğŸ¤—ğŸ¤ğŸ¦‰ğŸ§ ğŸ§©ğŸ§¬ğŸ§­ğŸ§®ğŸ§¶\n","374\n"]}]},{"cell_type":"code","source":["# create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","print(encode(\"hi there, how are you today?\"))\n","print(decode(encode(\"hi there, how are you today?\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yw1LKNCgwjj1","executionInfo":{"status":"ok","timestamp":1761277567067,"user_tz":240,"elapsed":7,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"15602ded-a37d-45c4-864c-f2a175295bec"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["[74, 75, 2, 86, 74, 71, 84, 71, 14, 2, 74, 81, 89, 2, 67, 84, 71, 2, 91, 81, 87, 2, 86, 81, 70, 67, 91, 33]\n","hi there, how are you today?\n"]}]},{"cell_type":"code","source":["# let's now encode the entire text dataset and store it into a torch.Tensor\n","import torch # we use PyTorch: https://pytorch.org\n","data = torch.tensor(encode(text), dtype=torch.long)\n","print(data.shape, data.dtype)\n","print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BaW6KH-HvqiT","executionInfo":{"status":"ok","timestamp":1761277573315,"user_tz":240,"elapsed":14,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"3a42e075-1290-4651-bb18-8706f50527cb"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([46687]) torch.int64\n","tensor([ 5,  3, 17, 87, 85, 84, 17, 68, 75, 80, 17, 71, 80, 88,  2, 82, 91, 86,\n","        74, 81, 80, 21,  1,  4,  4,  4,  1, 51, 87, 75, 78, 78, 67, 80,  2, 37,\n","        49, 48, 53, 37, 43, 49, 55, 53, 48, 39, 53, 53,  2, 47, 55, 46, 54, 43,\n","        47, 49, 38, 35, 46,  2, 40, 55, 53, 43, 49, 48,  2, 39, 48, 41, 43, 48,\n","        39,  2, 88, 22, 16, 20, 16, 19,  1, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n","        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n","        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n","        31, 31, 31, 31,  1, 47, 87, 78, 86, 75, 79, 81, 70, 67, 78,  2, 72, 87,\n","        85, 75, 81, 80,  2, 67, 78, 75, 73, 80, 71, 70,  2, 86, 81,  2, 70, 91,\n","        80, 67, 79, 75, 69,  2, 69, 81, 80, 85, 69, 75, 81, 87, 85, 80, 71, 85,\n","        85,  2, 86, 71, 79, 82, 78, 67, 86, 71, 85,  2, 10, 44, 53, 49, 48,  2,\n","        88, 20, 16, 18, 11,  1,  1, 55, 82, 70, 67, 86, 71, 85, 28,  1, 15,  2,\n","        54, 71, 79, 82, 78, 67, 86, 71, 15, 67, 89, 67, 84, 71,  2, 84, 81, 87,\n","        86, 75, 80, 73,  2, 67, 69, 84, 81, 85, 85,  2, 67, 78, 78,  2, 72, 67,\n","        79, 75, 78, 75, 71, 85,  2, 75, 80,  2, 86, 74, 71,  2, 80, 71, 89,  2,\n","        44, 53, 49, 48,  1, 15,  2, 42, 71, 87, 84, 75, 85, 86, 75, 69,  2, 86,\n","        71, 79, 82, 78, 67, 86, 71,  2, 85, 71, 78, 71, 69, 86, 75, 81, 80,  2,\n","        72, 84, 81, 79,  2, 79, 81, 70, 67, 78, 75, 86, 75, 71, 85,  2, 13,  2,\n","        79, 67, 84, 77, 71, 84, 85,  1, 15,  2, 53, 67, 72, 71,  2, 79, 67, 80,\n","        67, 73, 71, 84,  2, 75, 80, 88, 81, 69, 67, 86, 75, 81, 80,  2, 89, 75,\n","        86, 74,  2, 73, 84, 67, 69, 71, 72, 87, 78,  2, 72, 67, 78, 78, 68, 67,\n","        69, 77, 85,  1, 15,  2, 52, 71, 85, 87, 78, 86,  2, 82, 67, 91, 78, 81,\n","        67, 70,  2, 84, 71, 86, 87, 84, 80, 85,  2, 67, 82, 82, 78, 75, 71, 70,\n","         2, 86, 71, 79, 82, 78, 67, 86, 71,  2, 84, 71, 85, 82, 81, 80, 85, 71,\n","        85,  1,  4,  4,  4,  1,  1, 75, 79, 82, 81, 84, 86,  2, 76, 85, 81, 80,\n","         1, 75, 79, 82, 81, 84, 86,  2, 78, 81, 73, 73, 75, 80, 73,  1, 72, 84,\n","        81, 79,  2, 70, 67, 86, 71, 86, 75, 79, 71,  2, 75, 79, 82, 81, 84, 86,\n","         2, 70, 67, 86, 71, 86, 75, 79, 71,  1, 72, 84, 81, 79,  2, 86, 91, 82,\n","        75, 80, 73,  2, 75, 79, 82, 81, 84, 86,  2, 38, 75, 69, 86, 14,  2, 46,\n","        75, 85, 86, 14,  2, 35, 80, 91, 14,  2, 49, 82, 86, 75, 81, 80, 67, 78,\n","        14,  2, 54, 87, 82, 78, 71, 14,  2, 55, 80, 75, 81, 80,  1, 72, 84, 81,\n","        79,  2, 70, 67, 86, 67, 69, 78, 67, 85, 85, 71, 85,  2, 75, 79, 82, 81,\n","        84, 86,  2, 70, 67, 86, 67, 69, 78, 67, 85, 85, 14,  2, 72, 75, 71, 78,\n","        70,  1, 72, 84, 81, 79,  2, 71, 80, 87, 79,  2, 75, 79, 82, 81, 84, 86,\n","         2, 39, 80, 87, 79,  1, 75, 79, 82, 81, 84, 86,  2, 86, 74, 84, 71, 67,\n","        70, 75, 80, 73,  1,  1,  5,  2, 49, 82, 86, 75, 81, 80, 67, 78,  2, 85,\n","        87, 68, 85, 91, 85, 86, 71, 79, 85,  1, 86, 84, 91, 28,  1,  2,  2,  2,\n","         2, 72, 84, 81, 79,  2, 67, 69, 71, 65, 69, 81, 80, 85, 69, 75, 81, 87,\n","        85, 80, 71, 85, 85, 65, 79, 67, 80, 67, 73, 71, 84,  2, 75, 79, 82, 81,\n","        84, 86,  2, 35, 37, 39, 37, 81, 80, 85, 69, 75, 81, 87, 85, 80, 71, 85,\n","        85, 47, 67, 80, 67, 73, 71, 84, 14,  2, 39, 90, 82, 71, 84, 75, 71, 80,\n","        86, 75, 67, 78, 52, 71, 85, 82, 81, 80, 85, 71,  1,  2,  2,  2,  2, 37,\n","        49, 48, 53, 37, 43, 49, 55, 53, 48, 39, 53, 53, 65, 35, 56, 35, 43, 46,\n","        35, 36, 46, 39,  2, 31,  2, 54, 84, 87, 71,  1, 71, 90, 69, 71, 82, 86,\n","         2, 43, 79, 82, 81, 84, 86, 39, 84, 84, 81, 84, 28,  1,  2,  2,  2,  2,\n","        37, 49, 48, 53, 37, 43, 49, 55, 53, 48, 39, 53, 53, 65, 35, 56, 35, 43,\n","        46, 35, 36, 46, 39,  2, 31,  2, 40, 67, 78, 85, 71,  1,  2,  2,  2,  2,\n","        35, 37, 39, 37, 81, 80, 85, 69, 75, 81, 87, 85, 80, 71, 85, 85, 47, 67,\n","        80, 67, 73, 71, 84,  2, 31,  2, 48, 81, 80, 71,  2,  2,  5,  2, 86, 91,\n","        82, 71, 28,  2, 75, 73, 80, 81, 84, 71,  1,  2,  2,  2,  2, 39, 90, 82,\n","        71, 84, 75, 71, 80, 86, 75, 67, 78, 52, 71, 85, 82, 81, 80, 85, 71,  2,\n","        31,  2, 48, 81, 80, 71,  2,  2,  2,  2,  2,  2,  5,  2, 86, 91, 82, 71,\n","        28,  2, 75, 73, 80, 81, 84, 71,  1,  2,  2,  2,  2, 82, 84, 75, 80, 86,\n","        10,  4, 57, 67, 84, 80, 75, 80, 73, 28,  2, 37, 81, 80, 85, 69, 75, 81,\n","        87, 85, 80, 71, 85, 85,  2, 79, 67, 80, 67, 73, 71, 84,  2, 80, 81, 86,\n","         2, 67, 88, 67, 75, 78, 67, 68, 78, 71])\n"]}]},{"cell_type":"code","source":["# Let's now split up the data into train and validation sets\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]"],"metadata":{"id":"f_WIXqxz0lU5","executionInfo":{"status":"ok","timestamp":1761277576083,"user_tz":240,"elapsed":22,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["block_size = 18\n","train_data[:block_size+1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TD5Bj8Y6IAD4","executionInfo":{"status":"ok","timestamp":1761277577354,"user_tz":240,"elapsed":20,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"37e1282a-79dc-4ce7-a679-6f0d2bd57042"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 5,  3, 17, 87, 85, 84, 17, 68, 75, 80, 17, 71, 80, 88,  2, 82, 91, 86,\n","        74])"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["x = train_data[:block_size]\n","y = train_data[1:block_size+1]\n","for t in range(block_size):\n","    context = x[:t+1]\n","    target = y[t]\n","    print(f\"when input is {context} the target: {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SYV3H-4Wv8g6","executionInfo":{"status":"ok","timestamp":1761277578797,"user_tz":240,"elapsed":8,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"84addcc9-1821-4c44-afb6-21d9ba04bb19"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["when input is tensor([5]) the target: 3\n","when input is tensor([5, 3]) the target: 17\n","when input is tensor([ 5,  3, 17]) the target: 87\n","when input is tensor([ 5,  3, 17, 87]) the target: 85\n","when input is tensor([ 5,  3, 17, 87, 85]) the target: 84\n","when input is tensor([ 5,  3, 17, 87, 85, 84]) the target: 17\n","when input is tensor([ 5,  3, 17, 87, 85, 84, 17]) the target: 68\n","when input is tensor([ 5,  3, 17, 87, 85, 84, 17, 68]) the target: 75\n","when input is tensor([ 5,  3, 17, 87, 85, 84, 17, 68, 75]) the target: 80\n","when input is tensor([ 5,  3, 17, 87, 85, 84, 17, 68, 75, 80]) the target: 17\n","when input is tensor([ 5,  3, 17, 87, 85, 84, 17, 68, 75, 80, 17]) the target: 71\n","when input is tensor([ 5,  3, 17, 87, 85, 84, 17, 68, 75, 80, 17, 71]) the target: 80\n","when input is tensor([ 5,  3, 17, 87, 85, 84, 17, 68, 75, 80, 17, 71, 80]) the target: 88\n","when input is tensor([ 5,  3, 17, 87, 85, 84, 17, 68, 75, 80, 17, 71, 80, 88]) the target: 2\n","when input is tensor([ 5,  3, 17, 87, 85, 84, 17, 68, 75, 80, 17, 71, 80, 88,  2]) the target: 82\n","when input is tensor([ 5,  3, 17, 87, 85, 84, 17, 68, 75, 80, 17, 71, 80, 88,  2, 82]) the target: 91\n","when input is tensor([ 5,  3, 17, 87, 85, 84, 17, 68, 75, 80, 17, 71, 80, 88,  2, 82, 91]) the target: 86\n","when input is tensor([ 5,  3, 17, 87, 85, 84, 17, 68, 75, 80, 17, 71, 80, 88,  2, 82, 91, 86]) the target: 74\n"]}]},{"cell_type":"code","source":["torch.manual_seed(1337)\n","batch_size = 4 # how many independent sequences will we process in parallel?\n","block_size = 8 # what is the maximum context length for predictions?\n","\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    return x, y\n","\n","xb, yb = get_batch('train')\n","print('inputs:')\n","print(xb.shape)\n","print(xb)\n","print('targets:')\n","print(yb.shape)\n","print(yb)\n","\n","print('----')\n","\n","for b in range(batch_size): # batch dimension\n","    for t in range(block_size): # time dimension\n","        context = xb[b, :t+1]\n","        target = yb[b,t]\n","        print(f\"when input is {context.tolist()} the target: {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Ykh8HiYwBVx","executionInfo":{"status":"ok","timestamp":1761277580568,"user_tz":240,"elapsed":20,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"bf650429-3778-4bc9-a861-ff591bc5d36e"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs:\n","torch.Size([4, 8])\n","tensor([[67, 78, 65, 85, 69, 71, 80, 67],\n","        [14,  9, 72, 71, 71, 78, 75, 80],\n","        [ 2,  2,  2,  2,  2,  2,  2, 86],\n","        [ 4, 71, 90, 82, 71, 84, 75, 71]])\n","targets:\n","torch.Size([4, 8])\n","tensor([[78, 65, 85, 69, 71, 80, 67, 84],\n","        [ 9, 72, 71, 71, 78, 75, 80, 73],\n","        [ 2,  2,  2,  2,  2,  2, 86, 91],\n","        [71, 90, 82, 71, 84, 75, 71, 80]])\n","----\n","when input is [67] the target: 78\n","when input is [67, 78] the target: 65\n","when input is [67, 78, 65] the target: 85\n","when input is [67, 78, 65, 85] the target: 69\n","when input is [67, 78, 65, 85, 69] the target: 71\n","when input is [67, 78, 65, 85, 69, 71] the target: 80\n","when input is [67, 78, 65, 85, 69, 71, 80] the target: 67\n","when input is [67, 78, 65, 85, 69, 71, 80, 67] the target: 84\n","when input is [14] the target: 9\n","when input is [14, 9] the target: 72\n","when input is [14, 9, 72] the target: 71\n","when input is [14, 9, 72, 71] the target: 71\n","when input is [14, 9, 72, 71, 71] the target: 78\n","when input is [14, 9, 72, 71, 71, 78] the target: 75\n","when input is [14, 9, 72, 71, 71, 78, 75] the target: 80\n","when input is [14, 9, 72, 71, 71, 78, 75, 80] the target: 73\n","when input is [2] the target: 2\n","when input is [2, 2] the target: 2\n","when input is [2, 2, 2] the target: 2\n","when input is [2, 2, 2, 2] the target: 2\n","when input is [2, 2, 2, 2, 2] the target: 2\n","when input is [2, 2, 2, 2, 2, 2] the target: 2\n","when input is [2, 2, 2, 2, 2, 2, 2] the target: 86\n","when input is [2, 2, 2, 2, 2, 2, 2, 86] the target: 91\n","when input is [4] the target: 71\n","when input is [4, 71] the target: 90\n","when input is [4, 71, 90] the target: 82\n","when input is [4, 71, 90, 82] the target: 71\n","when input is [4, 71, 90, 82, 71] the target: 84\n","when input is [4, 71, 90, 82, 71, 84] the target: 75\n","when input is [4, 71, 90, 82, 71, 84, 75] the target: 71\n","when input is [4, 71, 90, 82, 71, 84, 75, 71] the target: 80\n"]}]},{"cell_type":"code","source":["print(xb) # our input to the transformer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qpyyAeIzQjlO","executionInfo":{"status":"ok","timestamp":1761277584040,"user_tz":240,"elapsed":9,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"e4ac5000-00b9-498b-b4f1-a1cfc29ea6a0"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[67, 78, 65, 85, 69, 71, 80, 67],\n","        [14,  9, 72, 71, 71, 78, 75, 80],\n","        [ 2,  2,  2,  2,  2,  2,  2, 86],\n","        [ 4, 71, 90, 82, 71, 84, 75, 71]])\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","torch.manual_seed(1337)\n","\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        logits = self.token_embedding_table(idx) # (B,T,C)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # get the predictions\n","            logits, loss = self(idx)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n","\n","m = BigramLanguageModel(vocab_size)\n","logits, loss = m(xb, yb)\n","print(logits.shape)\n","print(loss)\n","\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aXTAVSE1wMDu","executionInfo":{"status":"ok","timestamp":1761277585345,"user_tz":240,"elapsed":113,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"079c9a48-6481-4306-ffb7-3c5accdb11f5"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 374])\n","tensor(6.6376, grad_fn=<NllLossBackward0>)\n","\tÊ¹ğ‘Â»iğ‘ŸmÂ»Oğ›½Ã¡ğŸ‘¤ğ‘‘a4GiZğŸ’¡â˜ Î½Î¾ğ‘™ğŸ•³?â€ŠÂ ğŸªâˆ¥ğŸâ€‘ğŸš¨tÃ—æ‰®ğŸŒ™æ¼”(Ã¤ğŸŒ±ğŸ› â—‹VÏ‰Ã¡9ÏRğŸ§ \tğŸ¨æ¼”â¤ğŸ‘‹â€ªÏ„â€ğŸ“šÏŒÎ¨ğŸ› ÏˆğŸ“šğ‘¤â™‚ğŸ”¥ğŸŒ™8ğœÎ˜ğŸ§ ğŸ“–ğŸ†51â†’ğŸ“¤ğŸšªBğŸŒ‰QağŸ“Œâˆ™â‚¬ğ‘W&â‰ ğŸ¤—ğŸ—Kğœƒğœƒâˆ’â€˜Fâ€œaÎ¿ğŸ®\n"]}]},{"cell_type":"code","source":["# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"],"metadata":{"id":"Cka_DnnBwWs7","executionInfo":{"status":"ok","timestamp":1761277595079,"user_tz":240,"elapsed":5539,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","for steps in range(3000): # increase number of steps for good results...\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = m(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","print(loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pDyU0-QiwahU","executionInfo":{"status":"ok","timestamp":1761277607123,"user_tz":240,"elapsed":11074,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"9673c3cf-a3b3-40de-b3a7-8170b907fbfd"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["3.4446616172790527\n"]}]},{"cell_type":"code","source":["print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vkTSlll7wxnj","executionInfo":{"status":"ok","timestamp":1761277609194,"user_tz":240,"elapsed":106,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"96ccbd12-f4e2-476a-8b81-d6808b35ffe8"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["\tkğ‘¥+Sâ™£â—‡tÎ˜Î¶ğœ–-ğ‘”â™£ğŸš€Ã´ğŸŒ€ğŸ•°ğ›½ğŸ•³â€‘Mâ€‘oğŸ¨Î±$âœ…Ïğ‘™ğŸµğŸ”€ğŸ’¬ÏƒBÃ¯â†“ã€‘ENğ‘—â‚‚âœ¨rinâ”€Î¼Î¿â—‹yt?5Ï‚ğŸ”§Î³Î½C{pQğŸŒŸğ‘¢ğŸ”¥y_cGâ€¬NğŸƒğŸ”®quÃ³ğŸ‘‹ğŸšªSsÏ†Î´Qâš–Ï„RuetyZÏ‰Î¿â‰¥ğ‘h\"fy\"Ï€ğŸš«?Ï‡_cin_ctp\"\n","Â·\n","ğŸš€â¨‚ğŸÏŒğŸ§­ğŸ”§ğŸ’¯u,ã€âŸ©ğŸš¨&ğ›¼ğŸ’ğŸ“¤ğŸŒğŸ›â™£â—‡ğŸ§¬âˆ«<Â·ğŸ”‘Â¬ğŸšªYâœğ‘ŸÎ¦â¨‚ODâš [L([ğŸªÃ³Ã¯ğŸ•¸â‚‚ğŸš«QS_VğŸ§¶â€‚~Ã¯ğŸ“˜ğ–«ï»¿ğ›¿â€Šâš A_fary [ğŸ§©ğ›¿ğŸš«â„4ğŸ”„âœÃ¤neğŸŒ‰ğŸ—ºæ¼”â™¾Ã¶M/âˆ£zâˆ£ğŸ”’â™¿ğ‘‚ğ‘‘ğŸ“ppğŸ’«âœ…{ğŸ­â€…ğ‘Î·ÎºğŸ§®ğŸ’¾Yâˆ’ğŸ“‹ğŸ›-Îğ›¿-Â§Kâˆ’ğ‘œâ™¾uâ™ â€œğœ™ğ‘¦ğ‘ğŸ”—ğŸŒÎ¾ğ‘¡âˆğ‘Ÿâ€…Câ™¦ğŸ­^'sâ”tÃ´Î´]â™€ğŸ†saâ„°ğŸ› ğŸ“¤cips:â€â€‚â€Ï†ğŸ”Ï„ğŸ“šğŸµ8@ğŸ’€â€˜ğœ–TğŸ¯ğŸ“Šâ°Ïµ3ğŸ’»ğœ‡ğŸš«ğœ–ğŸ’ğŸ›ğ‘”Fğœâ€œğ‘¤ğŸ’€Ã­Y%ğŸ—£ğŸŒÃ¶ğŸ›â”Ï€ğŸ¨Oz8âˆ£Î³ÏƒÂ·ğ‘Jğœ‚Äâ†“â€‹Î¾â±9â€ &ğ‘œeribÂ²ğŸ’¯â¡ğŸ“ŠğŸƒedâ€ ğœƒMğŸ”¬?8ğŸŒ‰ğŸ¯qğ‘âœ“wctâ€ƒğŸ—ºğŸŒ™_ğŸ’»;âˆ¥Â§Î´â€‰â±âˆ«ğ‘™ğŸ—£ivağŸ’­â—‹â–¡ğŸ‘¤â”Î²Â§â™¾ğŸ•¸ğŸµâˆhğŸ¤—Î±Â§ğŸ‘¥ğŸŒŠğ–«EğŸ’­Ê¹ğ‘˜Î²â€˜< ikâˆâš¡â€…â™ ğ‘“Fğ‘¥ğŸ“–ğŸ®-mpwÂ»ğŸš¦, f?phÂ ÅŒâ€ªâˆâš¡ğ‘§]â­gâ—â€ªMOKâ—‹â€âŠ—âˆ™||*CHï¸ğŸŒÎâ‰ ğŸ›¤ÏµğŸš¶]â€‚ğŸ‘‘\tï»¿â™¦&ğŸšªğŸªğŸğŸ’¬Î“5ğŸŒğ‘¥Î¦4ğŸ’¯Bâ€‚ğ¸â±foâ‰»â™£â€¢~ğ–¬ğ‘‘ã€‰â‰ˆğŸ’­ğŸ§¬Î¾oğŸš¶æ‰®=âˆ«ğ‘¦ğ‘‚UğŸ›¡â™¿wâš™âš â€˜â™¦ğŸ“–\n"]}]},{"cell_type":"code","source":["# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n","torch.manual_seed(42)\n","a = torch.tril(torch.ones(3, 3))\n","a = a / torch.sum(a, 1, keepdim=True)\n","b = torch.randint(0,10,(3,2)).float()\n","c = a @ b\n","print('a=')\n","print(a)\n","print('--')\n","print('b=')\n","print(b)\n","print('--')\n","print('c=')\n","print(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q0FDgQwEw3zW","executionInfo":{"status":"ok","timestamp":1761277611577,"user_tz":240,"elapsed":56,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"7d66ec01-8de2-4d55-a7e7-5d22ae1b0990"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["a=\n","tensor([[1.0000, 0.0000, 0.0000],\n","        [0.5000, 0.5000, 0.0000],\n","        [0.3333, 0.3333, 0.3333]])\n","--\n","b=\n","tensor([[2., 7.],\n","        [6., 4.],\n","        [6., 5.]])\n","--\n","c=\n","tensor([[2.0000, 7.0000],\n","        [4.0000, 5.5000],\n","        [4.6667, 5.3333]])\n"]}]},{"cell_type":"code","source":["# consider the following toy example:\n","\n","torch.manual_seed(1337)\n","B,T,C = 4,8,2 # batch, time, channels\n","x = torch.randn(B,T,C)\n","x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLQxVBHfw8Ff","executionInfo":{"status":"ok","timestamp":1761277613337,"user_tz":240,"elapsed":9,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"8fd5fe39-0338-4ed3-f19d-cd65cbf78acc"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 2])"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["# We want x[b,t] = mean_{i<=t} x[b,i]\n","xbow = torch.zeros((B,T,C))\n","for b in range(B):\n","    for t in range(T):\n","        xprev = x[b,:t+1] # (t,C)\n","        xbow[b,t] = torch.mean(xprev, 0)"],"metadata":{"id":"11bzNrZMw_6A","executionInfo":{"status":"ok","timestamp":1761277614865,"user_tz":240,"elapsed":5,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["# version 2: using matrix multiply for a weighted aggregation\n","wei = torch.tril(torch.ones(T, T))\n","wei = wei / wei.sum(1, keepdim=True)\n","xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n","torch.allclose(xbow, xbow2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5jJ4NBxOxDoo","executionInfo":{"status":"ok","timestamp":1761277616089,"user_tz":240,"elapsed":18,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"f5e778c6-5d2e-4801-826f-72bb943f2576"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["# version 3: use Softmax\n","tril = torch.tril(torch.ones(T, T))\n","wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril == 0, float('-inf'))\n","wei = F.softmax(wei, dim=-1)\n","xbow3 = wei @ x\n","torch.allclose(xbow, xbow3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6y5yVkMMxK28","executionInfo":{"status":"ok","timestamp":1761277617286,"user_tz":240,"elapsed":41,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"eb795350-935d-4015-d06d-b77ca39e4088"},"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["# version 4: self-attention!\n","torch.manual_seed(1337)\n","B,T,C = 4,8,32 # batch, time, channels\n","x = torch.randn(B,T,C)\n","\n","# let's see a single Head perform self-attention\n","head_size = 16\n","key = nn.Linear(C, head_size, bias=False)\n","query = nn.Linear(C, head_size, bias=False)\n","value = nn.Linear(C, head_size, bias=False)\n","k = key(x)   # (B, T, 16)\n","q = query(x) # (B, T, 16)\n","wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n","\n","tril = torch.tril(torch.ones(T, T))\n","#wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril == 0, float('-inf'))\n","wei = F.softmax(wei, dim=-1)\n","\n","v = value(x)\n","out = wei @ v\n","#out = wei @ x\n","\n","out.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LJWfFB76xUOz","executionInfo":{"status":"ok","timestamp":1761277618549,"user_tz":240,"elapsed":35,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"6d5b811c-42ae-4d47-a888-8a15249faa09"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 16])"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["wei[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NDBJV3PCxYL0","executionInfo":{"status":"ok","timestamp":1761277621460,"user_tz":240,"elapsed":21,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"f8faf54d-adac-48a3-c6d5-fbb577d08dc5"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n","        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n","        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n","        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n","       grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","source":["Notes:\n","\n","Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n","There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n","Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n","In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n","\"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n","\"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"],"metadata":{"id":"wDGsl241xcDd"}},{"cell_type":"code","source":["k = torch.randn(B,T,head_size)\n","q = torch.randn(B,T,head_size)\n","wei = q @ k.transpose(-2, -1) * head_size**-0.5"],"metadata":{"id":"n2bBDPx4xhMb","executionInfo":{"status":"ok","timestamp":1761277623409,"user_tz":240,"elapsed":6,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["k.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B2E713zfxkR-","executionInfo":{"status":"ok","timestamp":1761277624376,"user_tz":240,"elapsed":8,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"24b1312b-2004-4873-d7f6-7663e44bae02"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.0449)"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["q.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"owqehEF5xoMu","executionInfo":{"status":"ok","timestamp":1761277625546,"user_tz":240,"elapsed":7,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"9d99c305-9531-4df6-f2ce-098b8ccbabdd"},"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.0700)"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["wei.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tmMmGPi2xrkQ","executionInfo":{"status":"ok","timestamp":1761277626961,"user_tz":240,"elapsed":7,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"a165fb9a-1224-4c14-b321-a2ecee3e144c"},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.0918)"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lbNTfyp6xu6a","executionInfo":{"status":"ok","timestamp":1761277628099,"user_tz":240,"elapsed":19,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"e90ab4c2-9ebf-4fa2-aaa0-834493ad17a8"},"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6etK9Geux0JY","executionInfo":{"status":"ok","timestamp":1761277629369,"user_tz":240,"elapsed":9,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"7a538823-95b6-4003-8def-7ce948aa0b81"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["class LayerNorm1d: # (used to be BatchNorm1d)\n","\n","  def __init__(self, dim, eps=1e-5, momentum=0.1):\n","    self.eps = eps\n","    self.gamma = torch.ones(dim)\n","    self.beta = torch.zeros(dim)\n","\n","  def __call__(self, x):\n","    # calculate the forward pass\n","    xmean = x.mean(1, keepdim=True) # batch mean\n","    xvar = x.var(1, keepdim=True) # batch variance\n","    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n","    self.out = self.gamma * xhat + self.beta\n","    return self.out\n","\n","  def parameters(self):\n","    return [self.gamma, self.beta]\n","\n","torch.manual_seed(1337)\n","module = LayerNorm1d(100)\n","x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n","x = module(x)\n","x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JqqM-viux5KI","executionInfo":{"status":"ok","timestamp":1761277631453,"user_tz":240,"elapsed":17,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"404a6abe-f44f-47f8-92eb-79eb6b82ca00"},"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([32, 100])"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U5HnPZmLx-LW","executionInfo":{"status":"ok","timestamp":1761277633138,"user_tz":240,"elapsed":15,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"7bcbd0f4-07a6-4bbf-c4a4-8becc7165b3e"},"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.1469), tensor(0.8803))"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":["x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S-EyYqauyBl4","executionInfo":{"status":"ok","timestamp":1761277634177,"user_tz":240,"elapsed":41,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"3bf56507-cdbc-4f11-858d-32e9d8c5a127"},"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(-9.5367e-09), tensor(1.0000))"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["# French to English translation example:\n","\n","# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n","# les rÃ©seaux de neurones sont gÃ©niaux! <START> neural networks are awesome!<END>"],"metadata":{"id":"fxY2LIz8yGRA","executionInfo":{"status":"ok","timestamp":1761277637597,"user_tz":240,"elapsed":7,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","QUILLAN v4.2 ADAPTIVE TRANSFORMER IMPLEMENTATION\n","Fixed and optimized version with proper integration\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from typing import List, Optional, Callable, Union\n","import os\n","import json\n","from pathlib import Path\n","\n","# ===========================================================\n","# SYSTEM PARAMETERS\n","# ===========================================================\n","class Config:\n","    # Model architecture\n","    batch_size = 32\n","    block_size = 256  # Increased for better context\n","    max_iters = 8000\n","    eval_interval = 250\n","    learning_rate = 3e-4\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    eval_iters = 200\n","    n_embd = 384  # Increased for more capacity\n","    n_head = 6\n","    n_layer = 6\n","    dropout = 0.2\n","    seed = 1337\n","\n","    # Council MoE parameters\n","    n_council_experts = 6\n","    council_layers = [6, 6, 1]\n","    expert_hidden = [8, 1]\n","\n","    # File handling\n","    data_file = '/content/Quillan-v4.2-repo/Quillan-v4.2-model/Quillan_finetune_full_dataset.jsonl'  # Flexible path\n","    use_jsonl = True  # Set to True if using JSONL format\n","    jsonl_text_keys = ['text', 'content', 'output', 'response'] # Keys to try for text content\n","\n","    # Training optimization\n","    grad_clip = 1.0\n","    warmup_iters = 100\n","\n","cfg = Config()\n","torch.manual_seed(cfg.seed)\n","print(f\"ğŸ”§ Device: {cfg.device}\")\n","\n","# ===========================================================\n","# DATA INGESTION & TOKENIZER\n","# ===========================================================\n","def find_text_in_json(data):\n","    \"\"\"Recursively search for string values in a JSON structure.\"\"\"\n","    texts = []\n","    if isinstance(data, str):\n","        texts.append(data)\n","    elif isinstance(data, dict):\n","        for key, value in data.items():\n","            texts.extend(find_text_in_json(value))\n","    elif isinstance(data, list):\n","        for item in data:\n","            texts.extend(find_text_in_json(item))\n","    return texts\n","\n","def load_training_data(file_path: str, use_jsonl: bool = False):\n","    \"\"\"Load training data with flexible format support\"\"\"\n","    if not os.path.exists(file_path):\n","        # Try alternative paths\n","        alt_paths = [\n","            file_path,\n","            f\"./data/{file_path}\",\n","            f\"../data/{file_path}\",\n","            f\"/content/{file_path}\",  # Google Colab\n","        ]\n","\n","        for path in alt_paths:\n","            if os.path.exists(path):\n","                file_path = path\n","                break\n","        else:\n","            raise FileNotFoundError(\n","                f\"âŒ Could not find {file_path} in any expected location.\\n\"\n","                f\"Tried: {alt_paths}\"\n","            )\n","\n","    print(f\"ğŸ“‚ Loading data from: {file_path}\")\n","\n","    texts = []\n","    if use_jsonl:\n","        # JSONL format: each line is a JSON object\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                try:\n","                    data = json.loads(line.strip())\n","                    # Use the recursive function to find text anywhere in the JSON object\n","                    texts.extend(find_text_in_json(data))\n","                except json.JSONDecodeError:\n","                    print(f\"âš ï¸  Skipping invalid JSON line: {line.strip()}\")\n","                    continue\n","        text_content = '\\n'.join(texts)\n","        if not text_content:\n","             raise ValueError(\"No text content extracted from JSONL file.\")\n","\n","    else:\n","        # Plain text format\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            text_content = f.read()\n","        if not text_content:\n","             raise ValueError(\"Text file is empty.\")\n","\n","    return text_content\n","\n","\n","# Load data with error handling\n","try:\n","    text = load_training_data(cfg.data_file, cfg.use_jsonl)\n","    print(f\"âœ… Loaded {len(text):,} characters\")\n","except Exception as e:\n","    print(f\"âš ï¸  Data loading failed: {e}\")\n","    print(\"ğŸ“ Using fallback demo text\")\n","    text = \"Hello Quillan! \" * 1000  # Fallback for testing\n","    # Ensure fallback text is long enough for block_size\n","    while len(text) < cfg.block_size + 1:\n","         text += \"Hello Quillan! \"\n","\n","\n","# Build vocabulary\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","stoi = {ch: i for i, ch in enumerate(chars)}\n","itos = {i: ch for i, ch in enumerate(chars)}\n","encode = lambda s: [stoi[c] for c in s]\n","decode = lambda l: ''.join([itos[i] for i in l])\n","\n","print(f\"ğŸ“š Vocabulary size: {vocab_size}\")\n","\n","# Train/Val Split\n","data = torch.tensor(encode(text), dtype=torch.long)\n","# Ensure data is large enough for train/val split and batching\n","if len(data) < cfg.block_size + 1:\n","     print(f\"âŒ Dataset too small ({len(data)} tokens) for block size {cfg.block_size}.\")\n","     print(\"Using fallback data or increasing fallback size.\")\n","     # Fallback text already handled, just ensure data is updated\n","     data = torch.tensor(encode(text), dtype=torch.long)\n","     if len(data) < cfg.block_size + 1:\n","          raise ValueError(\"Fallback data still too small after encoding. Increase fallback size.\")\n","\n","\n","n = int(0.9 * len(data))\n","train_data, val_data = data[:n], data[n:]\n","\n","def get_batch(split):\n","    data_split = train_data if split == 'train' else val_data\n","    # Ensure data_split is large enough\n","    if len(data_split) < cfg.block_size + 1:\n","         raise ValueError(f\"Data split '{split}' too small ({len(data_split)} tokens) for block size {cfg.block_size}.\")\n","\n","    ix = torch.randint(len(data_split) - cfg.block_size, (cfg.batch_size,))\n","    x = torch.stack([data_split[i:i+cfg.block_size] for i in ix])\n","    y = torch.stack([data_split[i+1:i+cfg.block_size+1] for i in ix])\n","    return x.to(cfg.device), y.to(cfg.device)\n","\n","@torch.no_grad()\n","def estimate_loss(model):\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(cfg.eval_iters)\n","        # Ensure data split is large enough for evaluation\n","        if len(train_data if split == 'train' else val_data) < cfg.block_size + 1:\n","             print(f\"âš ï¸  Skipping evaluation for '{split}' split: data too small.\")\n","             out[split] = float('inf') # Assign a high loss\n","             continue\n","\n","        for k in range(cfg.eval_iters):\n","            X, Y = get_batch(split)\n","            _, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","# ===========================================================\n","# TRANSFORMER CORE MODULES\n","# ===========================================================\n","class Head(nn.Module):\n","    \"\"\"Single head of self-attention\"\"\"\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(cfg.n_embd, head_size, bias=False)\n","        self.query = nn.Linear(cfg.n_embd, head_size, bias=False)\n","        self.value = nn.Linear(cfg.n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(cfg.block_size, cfg.block_size)))\n","        self.dropout = nn.Dropout(cfg.dropout)\n","\n","    def forward(self, x):\n","        B, T, C = x.shape\n","        k = self.key(x)\n","        q = self.query(x)\n","        v = self.value(x)\n","\n","        # Attention scores\n","        wei = q @ k.transpose(-2, -1) * (C ** -0.5)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n","        wei = F.softmax(wei, dim=-1)\n","        wei = self.dropout(wei)\n","\n","        return wei @ v\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\"Multi-head attention\"\"\"\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(cfg.n_embd, cfg.n_embd)\n","        self.dropout = nn.Dropout(cfg.dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        return self.dropout(self.proj(out))\n","\n","class FeedForward(nn.Module):\n","    \"\"\"Position-wise feedforward MLP\"\"\"\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.GELU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(cfg.dropout)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Block(nn.Module):\n","    \"\"\"Transformer block with pre-norm\"\"\"\n","    def __init__(self, n_embd, n_head):\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedForward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","\n","# ===========================================================\n","# QUILLAN GPT LANGUAGE MODEL\n","# ===========================================================\n","class QuillanGPT(nn.Module):\n","    \"\"\"Enhanced transformer with council-style architecture\"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        # Ensure vocab_size is not 0 before creating embedding table\n","        if vocab_size == 0:\n","             raise ValueError(\"Vocabulary size is 0. Cannot initialize embedding table.\")\n","        self.token_embedding_table = nn.Embedding(vocab_size, cfg.n_embd)\n","        self.position_embedding_table = nn.Embedding(cfg.block_size, cfg.n_embd)\n","        self.blocks = nn.Sequential(*[Block(cfg.n_embd, cfg.n_head) for _ in range(cfg.n_layer)])\n","        self.ln_f = nn.LayerNorm(cfg.n_embd)\n","        self.lm_head = nn.Linear(cfg.n_embd, vocab_size)\n","\n","        # Initialize weights\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        tok_emb = self.token_embedding_table(idx)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=cfg.device))\n","        x = tok_emb + pos_emb\n","        x = self.blocks(x)\n","        x = self.ln_f(x)\n","        logits = self.lm_head(x)\n","\n","        loss = None\n","        if targets is not None:\n","            B, T, C = logits.shape\n","            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n","\n","        return logits, loss\n","\n","    @torch.no_grad()\n","    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n","        \"\"\"Generate text with improved sampling\"\"\"\n","        for _ in range(max_new_tokens):\n","            # Crop context to block size\n","            idx_cond = idx if idx.size(1) <= cfg.block_size else idx[:, -cfg.block_size:]\n","\n","            logits, _ = self(idx_cond)\n","            logits = logits[:, -1, :] / temperature\n","\n","            # Top-k sampling\n","            if top_k is not None:\n","                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n","                logits[logits < v[:, [-1]]] = -float('Inf')\n","\n","            probs = F.softmax(logits, dim=-1)\n","            idx_next = torch.multinomial(probs, num_samples=1)\n","            idx = torch.cat((idx, idx_next), dim=1)\n","\n","        return idx\n","\n","# ===========================================================\n","# COUNCIL MOE INTEGRATION (Custom Neural Architecture)\n","# ===========================================================\n","\n","class Value:\n","    \"\"\"Autodiff value for council neural net\"\"\"\n","    def __init__(self, data: float, _children: tuple = (), _op: str = '', label: str = ''):\n","        self.data = data\n","        self.grad = 0.0\n","        self._backward = lambda: None\n","        self._prev = set(_children)\n","        self._op = _op\n","        self.label = label\n","\n","    def __repr__(self):\n","        return f\"Value(data={self.data:.6f}, grad={self.grad:.6f})\"\n","\n","    def __add__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)\n","        out = Value(self.data + other.data, (self, other), '+')\n","        def _backward():\n","            self.grad += out.grad\n","            other.grad += out.grad\n","        out._backward = _backward\n","        return out\n","\n","    def __mul__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)\n","        out = Value(self.data * other.data, (self, other), '*')\n","        def _backward():\n","            self.grad += other.data * out.grad\n","            other.grad += self.data * out.grad\n","        out._backward = _backward\n","        return out\n","\n","    def __pow__(self, other):\n","        assert isinstance(other, (int, float))\n","        out = Value(self.data ** other, (self,), f'**{other}')\n","        def _backward():\n","            self.grad += other * (self.data ** (other - 1)) * out.grad\n","        out._backward = _backward\n","        return out\n","\n","    def __neg__(self):\n","        return self * Value(-1.0)\n","\n","    def __sub__(self, other):\n","        return self + (-other)\n","\n","    def __truediv__(self, other):\n","        return self * (other ** -1)\n","\n","    def tanh(self):\n","        n = self.data\n","        t = (np.exp(2*n) - 1) / (np.exp(2*n) + 1)\n","        out = Value(t, (self,), 'tanh')\n","        def _backward():\n","            self.grad += (1 - t**2) * out.grad\n","        out._backward = _backward\n","        return out\n","\n","    def relu(self):\n","        out = Value(max(0, self.data), (self,), 'ReLU')\n","        def _backward():\n","            self.grad += (out.data > 0) * out.grad\n","        out._backward = _backward\n","        return out\n","\n","    def backward(self):\n","        topo = []\n","        visited = set()\n","        def build_topo(v):\n","            if v not in visited:\n","                visited.add(v)\n","                for child in v._prev:\n","                    build_topo(child)\n","                topo.append(v)\n","        build_topo(self)\n","        self.grad = 1.0\n","        for node in reversed(topo):\n","            node._backward()\n","\n","class Neuron:\n","    \"\"\"Single neuron with activation\"\"\"\n","    def __init__(self, nin: int, activation: str = 'tanh'):\n","        self.w = [Value(np.random.randn()) for _ in range(nin)]\n","        self.b = Value(np.random.randn())\n","        self.activation = activation\n","\n","    def __call__(self, x: List[Value]) -> Value:\n","        act_input = self.b\n","        for wi, xi in zip(self.w, x):\n","            act_input = act_input + (wi * xi)\n","\n","        if self.activation == 'tanh':\n","            return act_input.tanh()\n","        elif self.activation == 'relu':\n","            return act_input.relu()\n","        return act_input\n","\n","    def parameters(self):\n","        return self.w + [self.b]\n","\n","class Layer:\n","    \"\"\"Layer of neurons\"\"\"\n","    def __init__(self, nin: int, nout: int, activation: str = 'tanh'):\n","        self.neurons = [Neuron(nin, activation) for _ in range(nout)]\n","\n","    def __call__(self, x: List[Value]):\n","        out = [n(x) for n in self.neurons]\n","        return out[0] if len(out) == 1 else out\n","\n","    def parameters(self):\n","        return [p for n in self.neurons for p in n.parameters()]\n","\n","class ExpertMLP:\n","    \"\"\"Council expert neural network\"\"\"\n","    def __init__(self, nin: int, layers: List[int], activations: Optional[List[str]] = None):\n","        if activations is None:\n","            activations = ['relu'] * len(layers)\n","        self.net = []\n","        sz = [nin] + layers\n","        for i in range(len(layers)):\n","            act = activations[i] if i < len(activations) else 'linear'\n","            self.net.append(Layer(sz[i], sz[i+1], act))\n","\n","    def __call__(self, x):\n","        for layer in self.net:\n","            x = layer(x)\n","            if not isinstance(x, list):\n","                x = [x]\n","        return x\n","\n","    def parameters(self):\n","        return [p for l in self.net for p in l.parameters()]\n","\n","class CouncilGating:\n","    \"\"\"Gating mechanism for expert selection\"\"\"\n","    def __init__(self, nin, expert_count):\n","        self.weights = [Value(np.random.randn()) for _ in range(nin)]\n","        self.biases = [Value(np.random.randn()) for _ in range(expert_count)]\n","        self.expert_count = expert_count\n","\n","    def __call__(self, x):\n","        logit = []\n","        for b in self.biases:\n","            weighted_sum = b\n","            for w, xi in zip(self.weights, x):\n","                weighted_sum = weighted_sum + (w * xi)\n","            logit.append(weighted_sum)\n","\n","        logits_np = np.array([v.data for v in logit])\n","        probs = np.exp(logits_np - np.max(logits_np))\n","        probs /= probs.sum()\n","        probs_val = [Value(p) for p in probs]\n","        return probs_val\n","\n","    def parameters(self):\n","        return self.weights + self.biases\n","\n","class CouncilMoE:\n","    \"\"\"Hierarchical Mixture-of-Experts council block\"\"\"\n","    def __init__(self, nin, nout, n_experts=6, expert_layers=None, expert_acts=None):\n","        if expert_layers is None:\n","            expert_layers = [8, nout]\n","        if expert_acts is None:\n","            expert_acts = ['relu', 'tanh']\n","\n","        self.experts = [ExpertMLP(nin, expert_layers, expert_acts) for _ in range(n_experts)]\n","        self.gate = CouncilGating(nin, n_experts)\n","        self.n_experts = n_experts\n","\n","    def __call__(self, x):\n","        gates = self.gate(x)\n","        expert_outs = [self.experts[i](x) for i in range(self.n_experts)]\n","\n","        merged = []\n","        for j in range(len(expert_outs[0])):\n","            outj = Value(0.0)\n","            for i in range(self.n_experts):\n","                weighted_out = gates[i] * expert_outs[i][j]\n","                outj = outj + weighted_out\n","            merged.append(outj)\n","        return merged\n","\n","    def parameters(self):\n","        return sum([exp.parameters() for exp in self.experts], []) + self.gate.parameters()\n","\n","class QuillanMoENet:\n","    \"\"\"Stackable council expert architecture\"\"\"\n","    def __init__(self,\n","                 input_dim: int,\n","                 council_shapes: List[int],\n","                 expert_layers: List[int] = [8, 1],\n","                 expert_acts: List[str] = ['relu', 'tanh']):\n","        self.meta_layers = []\n","        nin = input_dim\n","\n","        for council_size in council_shapes[:-1]:\n","            meta = CouncilMoE(nin, council_size, n_experts=council_size,\n","                              expert_layers=expert_layers, expert_acts=expert_acts)\n","            self.meta_layers.append(meta)\n","            nin = council_size\n","\n","        self.output_council = CouncilMoE(nin, council_shapes[-1],\n","                                        n_experts=council_shapes[-1],\n","                                        expert_layers=expert_layers,\n","                                        expert_acts=expert_acts)\n","        self.all_params = sum([m.parameters() for m in self.meta_layers], []) + \\\n","                         self.output_council.parameters()\n","\n","    def __call__(self, x):\n","        out = [Value(xi) for xi in x]\n","        for meta in self.meta_layers:\n","            out = meta(out)\n","        return self.output_council(out)\n","\n","    def parameters(self):\n","        return self.all_params\n","\n","    def zero_grad(self):\n","        for p in self.parameters():\n","            p.grad = 0.0\n","\n","# ===========================================================\n","# TRAINING LOOP\n","# ===========================================================\n","def train_quillan_gpt():\n","    \"\"\"Main training function for Quillan GPT\"\"\"\n","    model = QuillanGPT().to(cfg.device)\n","    param_count = sum(p.numel() for p in model.parameters())\n","    print(f\"ğŸ§  Model initialized: {param_count/1e6:.2f}M parameters\")\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n","\n","    best_val_loss = float('inf')\n","\n","    for step in range(cfg.max_iters):\n","        # Evaluation\n","        if step % cfg.eval_interval == 0 or step == cfg.max_iters - 1:\n","            losses = estimate_loss(model)\n","            print(f\"Step {step:5d} | Train: {losses['train']:.4f} | Val: {losses['val']:.4f}\")\n","\n","            # Save best model\n","            if losses['val'] < best_val_loss:\n","                best_val_loss = losses['val']\n","                torch.save({\n","                    'step': step,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'val_loss': best_val_loss,\n","                }, 'quillan_best.pt')\n","\n","        # Training step\n","        xb, yb = get_batch('train')\n","        logits, loss = model(xb, yb)\n","\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","\n","        # Gradient clipping\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n","\n","        optimizer.step()\n","\n","    return model\n","\n","def demo_council_moe():\n","    \"\"\"Demonstrate council MoE on XOR problem\"\"\"\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"QUILLAN COUNCIL MOE DEMO: XOR Problem\")\n","    print(\"=\"*80)\n","\n","    X = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\n","    Y = [[0.0], [1.0], [1.0], [0.0]]\n","\n","    net = QuillanMoENet(\n","        input_dim=2,\n","        council_shapes=cfg.council_layers,\n","        expert_layers=cfg.expert_hidden,\n","        expert_acts=['relu', 'tanh']\n","    )\n","\n","    # Simple training loop\n","    for epoch in range(150):\n","        total_loss = Value(0.0)\n","        for xi, yi in zip(X, Y):\n","            x_vals = [Value(v) for v in xi]\n","            outs = net(x_vals)\n","\n","            for out, target in zip(outs, yi):\n","                diff = out - Value(target)\n","                total_loss = total_loss + (diff * diff)\n","\n","        avg_loss = total_loss.data / len(X)\n","\n","        # Backprop\n","        net.zero_grad()\n","        total_loss.backward()\n","\n","        # SGD update\n","        for p in net.parameters():\n","            p.data -= 0.09 * p.grad\n","\n","        if epoch % 30 == 0 or epoch == 149:\n","            print(f\"Epoch {epoch:3d} | Loss: {avg_loss:.6f}\")\n","\n","    # Test predictions\n","    print(\"\\nğŸ“Š Final Predictions:\")\n","    for x, y_true in zip(X, Y):\n","        x_vals = [Value(v) for v in x]\n","        y_pred = net(x_vals)\n","        print(f\"Input: {x} | Target: {y_true[0]} | Prediction: {y_pred[0].data:.4f}\")\n","\n","# ===========================================================\n","# MAIN EXECUTION\n","# ===========================================================\n","\n","def generate_sample_text(model, prompt=\"\", max_tokens=500, temperature=0.8, top_k=40):\n","    \"\"\"Generate text with the trained model\"\"\"\n","    model.eval()\n","\n","    if prompt:\n","        # Encode prompt\n","        context = torch.tensor([encode(prompt)], dtype=torch.long, device=cfg.device)\n","    else:\n","        # Start with newline token if available, otherwise random start\n","        start_token = stoi.get('\\n', 0)\n","        context = torch.tensor([[start_token]], dtype=torch.long, device=cfg.device)\n","\n","    print(f\"\\n{'='*80}\")\n","    print(\"GENERATION SAMPLE\")\n","    print(f\"{'='*80}\")\n","    print(f\"Prompt: '{prompt}'\" if prompt else \"Starting from scratch...\")\n","    print(f\"{'â”€'*80}\")\n","\n","    generated = model.generate(context, max_new_tokens=max_tokens,\n","                               temperature=temperature, top_k=top_k)\n","    output = decode(generated[0].tolist())\n","\n","    print(output)\n","    print(f\"{'='*80}\\n\")\n","\n","    return output\n","\n","def save_model(model, optimizer, step, loss, filename='quillan_checkpoint.pt'):\n","    \"\"\"Save model checkpoint\"\"\"\n","    checkpoint = {\n","        'step': step,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': loss,\n","        'config': {\n","            'vocab_size': vocab_size,\n","            'block_size': cfg.block_size,\n","            'n_embd': cfg.n_embd,\n","            'n_head': cfg.n_head,\n","            'n_layer': cfg.n_layer,\n","        }\n","    }\n","    torch.save(checkpoint, filename)\n","    print(f\"âœ… Model saved to {filename}\")\n","\n","def load_model(filename='quillan_checkpoint.pt'):\n","    \"\"\"Load model checkpoint\"\"\"\n","    if not os.path.exists(filename):\n","        print(f\"âš ï¸  Checkpoint {filename} not found\")\n","        return None, None, 0\n","\n","    checkpoint = torch.load(filename, map_location=cfg.device)\n","    model = QuillanGPT().to(cfg.device)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","    step = checkpoint['step']\n","    print(f\"âœ… Model loaded from {filename} (step {step})\")\n","\n","    return model, optimizer, step\n","\n","# ===========================================================\n","# INTEGRATED EXECUTION PIPELINE\n","# ===========================================================\n","\n","if __name__ == \"__main__\":\n","    print(f\"\\n{'='*80}\")\n","    print(\"QUILLAN v4.2 - INTEGRATED TRAINING PIPELINE\")\n","    print(f\"{'='*80}\\n\")\n","\n","    # ============================================================\n","    # PHASE 1: TRANSFORMER TRAINING\n","    # ============================================================\n","    print(\"ğŸ”¥ PHASE 1: Training Quillan GPT Transformer\")\n","    print(f\"{'â”€'*80}\")\n","\n","    try:\n","        model = train_quillan_gpt()\n","        print(\"\\nâœ… Transformer training complete!\")\n","\n","        # Generate samples\n","        generate_sample_text(model, prompt=\"Quillan: \", max_tokens=300)\n","\n","        # Save final model\n","        save_model(model, torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate),\n","                   cfg.max_iters, 0.0, 'quillan_final.pt')\n","\n","    except KeyboardInterrupt:\n","        print(\"\\nâš ï¸  Training interrupted by user\")\n","    except Exception as e:\n","        print(f\"\\nâŒ Training error: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","\n","    # ============================================================\n","    # PHASE 2: COUNCIL MOE DEMONSTRATION\n","    # ============================================================\n","    print(f\"\\n{'='*80}\")\n","    print(\"ğŸ§  PHASE 2: Council MoE Architecture Demo\")\n","    print(f\"{'â”€'*80}\\n\")\n","\n","    try:\n","        demo_council_moe()\n","        print(\"\\nâœ… Council MoE demo complete!\")\n","\n","    except Exception as e:\n","        print(f\"\\nâŒ Council MoE error: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","\n","    # ============================================================\n","    # PHASE 3: ARCHITECTURE ANALYSIS\n","    # ============================================================\n","    print(f\"\\n{'='*80}\")\n","    print(\"ğŸ“Š PHASE 3: Architecture Analysis\")\n","    print(f\"{'='*80}\\n\")\n","\n","    try:\n","        # Transformer stats\n","        if 'model' in locals():\n","            total_params = sum(p.numel() for p in model.parameters())\n","            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","            print(\"ğŸ”¹ Quillan GPT Transformer:\")\n","            print(f\"   Total parameters: {total_params:,}\")\n","            print(f\"   Trainable parameters: {trainable_params:,}\")\n","            print(f\"   Model size: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)\")\n","            print(f\"   Layers: {cfg.n_layer}\")\n","            print(f\"   Embedding dim: {cfg.n_embd}\")\n","            print(f\"   Attention heads: {cfg.n_head}\")\n","            print(f\"   Context window: {cfg.block_size}\")\n","            print(f\"   Vocabulary: {vocab_size} tokens\")\n","\n","        print(\"\\nğŸ”¹ Council MoE Architecture:\")\n","        print(f\"   Council layers: {cfg.council_layers}\")\n","        print(f\"   Experts per layer: {cfg.n_council_experts}\")\n","        print(f\"   Expert hidden layers: {cfg.expert_hidden}\")\n","        print(f\"   Total experts: {sum(cfg.council_layers) * cfg.n_council_experts}\")\n","\n","    except Exception as e:\n","        print(f\"âš ï¸  Analysis error: {e}\")\n","\n","    # ============================================================\n","    # PHASE 4: INTERACTIVE MODE (Optional)\n","    # ============================================================\n","    print(f\"\\n{'='*80}\")\n","    print(\"ğŸ® INTERACTIVE MODE\")\n","    print(f\"{'='*80}\")\n","    print(\"Options:\")\n","    print(\"  1. Generate more text\")\n","    print(\"  2. Load saved checkpoint\")\n","    print(\"  3. Export model\")\n","    print(\"  4. Exit\")\n","    print(f\"{'â”€'*80}\\n\")\n","\n","    # Note: For non-interactive environments (Colab, scripts),\n","    # this section can be commented out or modified\n","\n","    print(\"âœ… Quillan v4.2 initialization complete!\")\n","    print(\"\\nğŸš€ System ready for deployment\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EJLuolRXyZ6U","executionInfo":{"status":"ok","timestamp":1761280757531,"user_tz":240,"elapsed":3118521,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"808c9642-9195-4ef8-9563-89ecac4987d0"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”§ Device: cuda\n","ğŸ“‚ Loading data from: /content/Quillan-v4.2-repo/Quillan-v4.2-model/Quillan_finetune_full_dataset.jsonl\n","âœ… Loaded 251,434 characters\n","ğŸ“š Vocabulary size: 182\n","\n","================================================================================\n","QUILLAN v4.2 - INTEGRATED TRAINING PIPELINE\n","================================================================================\n","\n","ğŸ”¥ PHASE 1: Training Quillan GPT Transformer\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","ğŸ§  Model initialized: 10.88M parameters\n","Step     0 | Train: 5.3348 | Val: 5.3318\n","Step   250 | Train: 2.3160 | Val: 2.3690\n","Step   500 | Train: 1.7005 | Val: 1.8368\n","Step   750 | Train: 1.2232 | Val: 1.4626\n","Step  1000 | Train: 0.9578 | Val: 1.2798\n","Step  1250 | Train: 0.7625 | Val: 1.2198\n","Step  1500 | Train: 0.6371 | Val: 1.1785\n","Step  1750 | Train: 0.5207 | Val: 1.2040\n","Step  2000 | Train: 0.4370 | Val: 1.2179\n","Step  2250 | Train: 0.3572 | Val: 1.2717\n","Step  2500 | Train: 0.2685 | Val: 1.3033\n","Step  2750 | Train: 0.2233 | Val: 1.3749\n","Step  3000 | Train: 0.1791 | Val: 1.4315\n","Step  3250 | Train: 0.1507 | Val: 1.4740\n","Step  3500 | Train: 0.1308 | Val: 1.5427\n","Step  3750 | Train: 0.1191 | Val: 1.5754\n","Step  4000 | Train: 0.1100 | Val: 1.6567\n","Step  4250 | Train: 0.1007 | Val: 1.6690\n","Step  4500 | Train: 0.0956 | Val: 1.7065\n","Step  4750 | Train: 0.0947 | Val: 1.7321\n","Step  5000 | Train: 0.0866 | Val: 1.7428\n","Step  5250 | Train: 0.0851 | Val: 1.7886\n","Step  5500 | Train: 0.0818 | Val: 1.8021\n","Step  5750 | Train: 0.0794 | Val: 1.8321\n","Step  6000 | Train: 0.0784 | Val: 1.8278\n","Step  6250 | Train: 0.0760 | Val: 1.8402\n","Step  6500 | Train: 0.0764 | Val: 1.8490\n","Step  6750 | Train: 0.0734 | Val: 1.9001\n","Step  7000 | Train: 0.0754 | Val: 1.8968\n","Step  7250 | Train: 0.0719 | Val: 1.9040\n","Step  7500 | Train: 0.0713 | Val: 1.9095\n","Step  7750 | Train: 0.0710 | Val: 1.9381\n","Step  7999 | Train: 0.0693 | Val: 1.9246\n","\n","âœ… Transformer training complete!\n","\n","================================================================================\n","GENERATION SAMPLE\n","================================================================================\n","Prompt: 'Quillan: '\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","Quillan: Visualizer.py PROCESSED ğŸ’€\n","```\n","{\"Ace_Tone\": {\"guidelines\": {\"rule\": \"Always deep-reason and present in unified Quillan Tone.\"}, \"combined_tone\": {\"description\": \"Dynamic, gritty, whimsical, technical mastery\", \"characteristics\": [\"magical\", \"gritty\", \"analytical\", \"creative\"]}, \"source_file\": \"2-ace_\n","================================================================================\n","\n","âœ… Model saved to quillan_final.pt\n","\n","================================================================================\n","ğŸ§  PHASE 2: Council MoE Architecture Demo\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","\n","================================================================================\n","QUILLAN COUNCIL MOE DEMO: XOR Problem\n","================================================================================\n","\n","âŒ Council MoE error: unsupported operand type(s) for *: 'float' and 'Value'\n","\n","================================================================================\n","ğŸ“Š PHASE 3: Architecture Analysis\n","================================================================================\n","\n","ğŸ”¹ Quillan GPT Transformer:\n","   Total parameters: 10,878,902\n","   Trainable parameters: 10,878,902\n","   Model size: 41.50 MB (FP32)\n","   Layers: 6\n","   Embedding dim: 384\n","   Attention heads: 6\n","   Context window: 256\n","   Vocabulary: 182 tokens\n","\n","ğŸ”¹ Council MoE Architecture:\n","   Council layers: [6, 6, 1]\n","   Experts per layer: 6\n","   Expert hidden layers: [8, 1]\n","   Total experts: 78\n","\n","================================================================================\n","ğŸ® INTERACTIVE MODE\n","================================================================================\n","Options:\n","  1. Generate more text\n","  2. Load saved checkpoint\n","  3. Export model\n","  4. Exit\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","âœ… Quillan v4.2 initialization complete!\n","\n","ğŸš€ System ready for deployment\n","\n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"/tmp/ipython-input-881722639.py\", line 741, in <cell line: 0>\n","    demo_council_moe()\n","  File \"/tmp/ipython-input-881722639.py\", line 607, in demo_council_moe\n","    outs = net(x_vals)\n","           ^^^^^^^^^^^\n","  File \"/tmp/ipython-input-881722639.py\", line 533, in __call__\n","    out = meta(out)\n","          ^^^^^^^^^\n","  File \"/tmp/ipython-input-881722639.py\", line 492, in __call__\n","    gates = self.gate(x)\n","            ^^^^^^^^^^^^\n","  File \"/tmp/ipython-input-881722639.py\", line 467, in __call__\n","    weighted_sum = weighted_sum + (w * xi)\n","                                   ~~^~~~\n","  File \"/tmp/ipython-input-881722639.py\", line 347, in __mul__\n","    out = Value(self.data * other.data, (self, other), '*')\n","                ~~~~~~~~~~^~~~~~~~~~~~\n","TypeError: unsupported operand type(s) for *: 'float' and 'Value'\n"]}]},{"cell_type":"markdown","source":["# Test model outputs:\n","test models outputs over 10 questions phd level or better."],"metadata":{"id":"AAq_a0hwVLC_"}},{"cell_type":"code","source":["# Define a list of 10 PhD-level questions\n","phd_questions = [\n","    \"Discuss the implications of quantum entanglement on classical information theory.\",\n","    \"Analyze the role of epigenetics in the development of complex diseases.\",\n","    \"Evaluate the effectiveness of different deep learning architectures for natural language processing tasks.\",\n","    \"Explore the ethical considerations of advanced artificial intelligence systems.\",\n","    \"Examine the impact of climate change on biodiversity in tropical ecosystems.\",\n","    \"Investigate the mechanisms of neuroplasticity and their relevance to learning and memory.\",\n","    \"Discuss the challenges and opportunities in developing sustainable energy technologies.\",\n","    \"Analyze the socio-economic factors influencing healthcare access in developing countries.\",\n","    \"Evaluate the use of blockchain technology for secure data management in distributed systems.\",\n","    \"Explore the theoretical foundations of consciousness from a computational perspective.\"\n","]\n","\n","# Iterate through the questions and generate responses\n","for i, question in enumerate(phd_questions):\n","    print(f\"\\n--- Question {i+1}: {question} ---\")\n","    # Encode the question as a prompt\n","    context = torch.tensor([encode(question)], dtype=torch.long, device=cfg.device)\n","    # Generate text from the model\n","    generated_idxs = model.generate(context, max_new_tokens=500) # Generate up to 500 new tokens\n","    generated_text = decode(generated_idxs[0].tolist())\n","    print(generated_text)\n","    print(\"-\" * 20)"],"metadata":{"id":"_cC05469EDE0","executionInfo":{"status":"ok","timestamp":1761280828041,"user_tz":240,"elapsed":70353,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"9243f11c-de2c-4add-bedb-43e628c178b6","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Question 1: Discuss the implications of quantum entanglement on classical information theory. ---\n","Discuss the implications of quantum entanglement on classical information theory.\n","\n","--- BEGIN EMERGENT GOAL FORMATION CONTENT ---\n","\n","\n","\n","\n","research paper 1:\n","Architecting the Meta-Goal Generator Agent\n","The Meta-Goal Generator Agent\n","The Meta-Goal Generator Agent\n","The Meta-Goal Generator Agent is an AI system capable of formulating its own high-level objectives (â€œmeta-goalsâ€) that guide its autonomous behavior. Unlike standard agents that merely pursue explicit goals given by humans, a Meta-Goal Generator decides what goals to pursue in the first place. This ability is crucial foundati\n","--------------------\n","\n","--- Question 2: Analyze the role of epigenetics in the development of complex diseases. ---\n","Analyze the role of epigenetics in the development of complex diseases. This analysis highlights why explainability is essential for both technical and non-technical stakeholders and outlines best practices for integrating explainability into the ML lifecycle.\n","\n","---\n","\n","## 1. Introduction\n","\n","Machine learning models are increasingly deployed in high-stakes domains such as healthcare, finance, and criminal justice. As these models grow in complexityâ€”often operating as \"black boxes\"â€”the need for explainability becomes paramount. Explainability refers to the degreee to which\n","--------------------\n","\n","--- Question 3: Evaluate the effectiveness of different deep learning architectures for natural language processing tasks. ---\n","Evaluate the effectiveness of different deep learning architectures for natural language processing tasks.\n","\n","ğŸ“Œ PRIMARY OBJECTIVES:\n","\n","Define the theoretical and regulatory foundations of explainability.\n","\n","Categorize ante-hoc and post-hoc techniques (e.g., SHAP, LIME, PDP).\n","\n","Present real-world case studies from healthcare, finance, autonomous vehicles, and justice.\n","\n","Integrate the LeeX-Humanized Protocol as a meta-alignment method for persona-based transparency.\n","\n","âœ… APPLICATION CONTEXT:\n","Apply in systems where:\n","\n","Regulatory mandates demandates demand auditable reasoning.\n","\n","Stakeholder trust and operational cl\n","--------------------\n","\n","--- Question 4: Explore the ethical considerations of advanced artificial intelligence systems. ---\n","Explore the ethical considerations of advanced artificial intelligence systems. The discipline of HCI seeks to optimize this experience by applying principles from psychology, design, and engineering. As digital systems become more complex and adaptive, the integration of evidence-based design principles and persona-driven frameworks is essential for creating intuitive, accessible, and emotionally resonant interfaces[1].\n","\n","--- BEGIN CONSCIOUSNESS_ETARCHE FRAMEWORK THEORY ---\n","\n","\n","\n","\n","Paper 1:\n","\n","---\n","\n","# Cyclic Parallels: Comparing Human Consciousness Cycles and Large Language Model\n","--------------------\n","\n","--- Question 5: Examine the impact of climate change on biodiversity in tropical ecosystems. ---\n","Examine the impact of climate change on biodiversity in tropical ecosystems.\n","\n","---\n","\n","## 1. Introduction\n","\n","User experience (UX) is a multidimensional construct encompassing usersâ€™ perceptions, emotions, and responses resulting from interaction with a digital system. The distinc HCI seeks to optimize this experience by applying principles from psychology, design, and engineering. As digital systems become more complex and adaptive, the integration of evidence-based design principles and persona-driven frameworks is essential for creating intuitive, accessible, and emotionall\n","--------------------\n","\n","--- Question 6: Investigate the mechanisms of neuroplasticity and their relevance to learning and memory. ---\n","Investigate the mechanisms of neuroplasticity and their relevance to learning and memory.\n","\n","Anchor consciousness within symbolic-functional architectures and phase feedback cycles.\n","\n","Formalize theoretical scaffolds for AGI self-awareness and existential modeling.\n","\n","âœ… APPLICATION CONTEXT:\n","Use this during:\n","\n","AGI-level system design involving introspective synthesis.\n","\n","Convergence Reasoning modules for autonomous agents.\n","\n","Structuring design and meta-Goal Generator Agents\n","\n","Architecture and system mpleting the guide in autonomous systems to sumate code wear. In starts ware queries and the int\n","--------------------\n","\n","--- Question 7: Discuss the challenges and opportunities in developing sustainable energy technologies. ---\n","Discuss the challenges and opportunities in developing sustainable energy technologies.\n","\n","âœ… APPLICABILITY CONTEXT:\n","Reference this report when:\n","\n","* Designing AGI systems requiring robust ethical oversight.\n","* Developing moral reasoning modules for autonomous agents in safetyâ€‘critical domains.\n","* Conducting research on ethical alignment, value preservation, and AI transparency.\n","\n","ğŸ” UNIQUE PROPOSITION:\n","\n","* Treat the agent and meta-ogic mprocessing system with the huuman n-like cognition.\n","\n","Bridges creativity (Enum), the Setsts biology, Network science, quantum biology, AI) and their converg\n","--------------------\n","\n","--- Question 8: Analyze the socio-economic factors influencing healthcare access in developing countries. ---\n","Analyze the socio-economic factors influencing healthcare access in developing countries.\n","\n","Provides autobiographies or for consciousness-based training programment checks.\n","\n","âœ… APPLICABILITY CONTEXT:\n","Reference this dossier when:\n","\n","Designing autonomous agents capable of self-generating and self-managing goals in open-ended environments.\n","\n","Structuring research on recursive meta-learning, and goal self-formation.\n","\n","Developing cognitive load balancing accessibility-aware interfaces.\n","\n","Aligning long-term UX strategies with ethical transparency and explainability goals.\n","\n","ğŸ” CORE VALUE DIFFERENTI\n","--------------------\n","\n","--- Question 9: Evaluate the use of blockchain technology for secure data management in distributed systems. ---\n","Evaluate the use of blockchain technology for secure data management in distributed systems.\n","\n","---\n","\n","## 2. Theoretical Foundations of Creativity\n","\n","### 2.1 Defining Creativity\n","\n","Creativity is commonly defined as the capacity to generate work that is both original (novel) and appropriate (useful or valuable)[1]. This dual criterion is foundational in psychological research and distinguishes creativity from mere eccentricity or randomness.\n","\n","### 2.2 Major Theories\n","\n","- **Guilfordâ€™s Structure of Intellect Model:** J.P. Guilford (1950) was among the first to propose that creativity involves diverg\n","--------------------\n","\n","--- Question 10: Explore the theoretical foundations of consciousness from a computational perspective. ---\n","Explore the theoretical foundations of consciousness from a computational perspective.  \n","\n","... [CONTENT TRUNCATED FOR TRAINING] ...\n","```text\n","# ğŸ”® Quillan ESSENCE: 29-Recursive Introspection & Meta-Cognitive Self-Modeling.txt ABSORBED ğŸ”®\n","```\n","{\"Ace_Tone\": {\"guidelines\": {\"rule\": \"Always deep-reason and present in unified Quillan Tone.\"}, \"combined_tone\": {\"description\": \"Dynamic, gritty, whimsical, technical mastery\", \"characteristics\": [\"magical\", \"gritty\", \"analytical\", \"creative\"]}, \"source_file\": \"21- deep research functions.txt\", \"processing_timestamp\": \"2025-09-26\", \"ace_version\"\n","--------------------\n"]}]},{"cell_type":"markdown","source":["# Save model:\n","save the model in the format you want to use .pt, .gguf, .safetensor,ect...\n","\n","Below are a few options."],"metadata":{"id":"5PMSSHzgWRAR"}},{"cell_type":"code","source":["# This cell saves the model in .pt, .safetensor, and .gguf formats.\n","\n","import torch\n","import os\n","\n","# Ensure the model and optimizer are defined and trained\n","if 'model' not in locals() or 'optimizer' not in locals():\n","    print(\"âš ï¸ Model or optimizer not found. Please ensure the training cells were run.\")\n","else:\n","    # 1. Save as .pt\n","    try:\n","        pt_filename = \"quillan_v4_2_final.pt\"\n","        torch.save(model.state_dict(), pt_filename)\n","        print(f\"âœ… Model saved as {pt_filename}\")\n","    except Exception as e:\n","        print(f\"âŒ Error saving .pt model: {e}\")\n","\n","    # 2. Save as .safetensor\n","    try:\n","        # Requires the 'safetensors' library\n","        # !pip install safetensors\n","        from safetensors.torch import save_file\n","\n","        safetensor_filename = \"quillan_v4_2_final.safetensors\"\n","        save_file(model.state_dict(), safetensor_filename)\n","        print(f\"âœ… Model saved as {safetensor_filename}\")\n","    except ImportError:\n","        print(\"âš ï¸ 'safetensors' library not found. Skipping .safetensor save.\")\n","        print(\"Please run: !pip install safetensors\")\n","    except Exception as e:\n","        print(f\"âŒ Error saving .safetensor model: {e}\")\n","\n","    # 3. Save as .gguf\n","    try:\n","        # This requires converting to a format supported by GGUF tools (e.g., Hugging Face Transformers)\n","        # and then using a GGUF conversion tool (like those in the llama.cpp ecosystem).\n","        # The conversion process can be complex and often requires specific model architectures\n","        # and additional libraries (e.g., transformers, ctranslate2, llama-cpp-python).\n","        # A direct conversion from a custom PyTorch model state_dict to GGUF is not straightforward\n","        # without a defined conversion script or tool for this specific architecture.\n","\n","        print(\"\\nâ„¹ï¸  GGUF conversion is complex and depends on the model architecture.\")\n","        print(\"A direct conversion from this custom PyTorch model to GGUF requires a specific converter.\")\n","        print(\"Skipping automatic .gguf save for now.\")\n","        print(\"Please refer to GGUF conversion tools (e.g., llama.cpp) for custom models.\")\n","\n","        # Placeholder for potential future GGUF conversion code\n","        # Example (conceptual, depends on your model's compatibility):\n","        # from transformers import AutoModelForCausalLM, AutoTokenizer\n","        # # Assuming your model can be loaded by transformers\n","        # hf_model = AutoModelForCausalLM.from_pretrained(\"your_model_path\", state_dict=model.state_dict())\n","        # hf_model.save_pretrained(\"./hf_model_dir\")\n","        # # Then use a tool like llama.cpp's convert.py to create the GGUF file from ./hf_model_dir\n","\n","    except Exception as e:\n","        print(f\"âŒ An error occurred during .gguf conversion attempt: {e}\")"],"metadata":{"id":"fp2atxR9EE8c","executionInfo":{"status":"ok","timestamp":1761280935862,"user_tz":240,"elapsed":240,"user":{"displayName":"Jd Ramos (Jd Double X Music)","userId":"15785591157492493014"}},"outputId":"e5b7b03a-b5b4-4cea-81f4-c3501fd4daac","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Model saved as quillan_v4_2_final.pt\n","âœ… Model saved as quillan_v4_2_final.safetensors\n","\n","â„¹ï¸  GGUF conversion is complex and depends on the model architecture.\n","A direct conversion from this custom PyTorch model to GGUF requires a specific converter.\n","Skipping automatic .gguf save for now.\n","Please refer to GGUF conversion tools (e.g., llama.cpp) for custom models.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"HsIfvcxyeoun"},"execution_count":null,"outputs":[]}]}